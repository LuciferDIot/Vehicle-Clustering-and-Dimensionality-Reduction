# -- To access eigenvalues and eigenvectors seperately.
arrests.eigen$values
arrests.eigen$vectors
# -- By default, eigenvectors in R point in the negative direction --
# -- Change the direction of the eigenvectors to positive side as it leads to more logical interpretation --
arrests.eigen$vectors <- -arrests.eigen$vectors
arrests.eigen$vectors
# -- Assign row and column names to the dataframe --
row.names(arrests.eigen$vectors) <- c("Murder", "Assault", "UrbanPop", "Rape")
colnames(arrests.eigen$vectors) <- c("PC1", "PC2", "PC3", "PC4")
arrests.eigen$vectors
# -- Selecting the Number of Principal Components --
# -- Calculate the Proportion of Variance Explained (PVE) --
arrests.eigen$values
PVE <- arrests.eigen$values / sum(arrests.eigen$values)
round(PVE, 2)
# PVE (aka scree) plot
PVEplot <- qplot(c(1:4), PVE) +
geom_line() +
xlab("Principal Component") +
ylab("PVE") +
ggtitle("Scree Plot") +
ylim(0, 1)
# Cumulative PVE plot
cumPVE <- qplot(c(1:4), cumsum(PVE)) +
geom_line() +
xlab("Principal Component") +
ylab(NULL) +
ggtitle("Cumulative Scree Plot") +
ylim(0,1)
grid.arrange(PVEplot, cumPVE, ncol = 2)
# -- Based on the PVE, select only the first two PCs --
phi <- arrests.eigen$vectors[,1:2]
phi
#Step 04 - Transfer datapoints to the new dimensional space
# -- Calculate Principal Components scores --
PC1 <- as.matrix(scaled_df) %*% phi[,1]       # %*% in R does matrix multiplication
PC2 <- as.matrix(scaled_df) %*% phi[,2]
# -- Create a dataframe with Principal Components scores --
PC <- data.frame(State = row.names(USArrests), PC1, PC2)
head(PC)
# -- Plot Principal Components for each State --
ggplot(PC, aes(PC1, PC2)) +
modelr::geom_ref_line(h = 0) +
modelr::geom_ref_line(v = 0) +
geom_text(aes(label = State), size = 3) +
xlab("First Principal Component") +
ylab("Second Principal Component") +
ggtitle("First Two Principal Components of USArrests Data")
install.packages("rlang")
install.packages("rlang")
install.packages("gridExtra")
install.packages("tidyverse")
install.packages("ggcorrplot")
install.packages("ggplot2")
install.packages("ggplot2")
##for data manipulation
library(tidyverse)
##arranging plots
library(gridExtra)
##plotting
library(ggplot2)
##computing with tidy data structures
library(rlang)
##visualizing correlation matrices
library(ggcorrplot)
# -- Load the dataset --
data("USArrests")
## the first six rows are displayed
head(USArrests)
#First we need to check whether we have to normalize the dataset or not.
#Compute variance (degree of spread in the dataset) of each variable
apply(USArrests, 2, var)
#Step 01 - Standardizing Values
# -- Method 01 --
scaled_df <- apply(USArrests, 2, scale)
head(scaled_df)
# -- Method 02 --
scaled_df <- scale(USArrests)
head(scaled_df)
#Step 02 - Compute the Covariance Matrix
arrests.cov <- cov(scaled_df)
arrests.cov
# -- Plot the Heat Map --
ggcorrplot(as.matrix((arrests.cov)))
#Step 03 - Calculate the eigenvalues & eigenvectors
arrests.eigen <- eigen(arrests.cov)
str(arrests.eigen)
# -- To access eigenvalues and eigenvectors seperately.
arrests.eigen$values
arrests.eigen$vectors
# -- By default, eigenvectors in R point in the negative direction --
# -- Change the direction of the eigenvectors to positive side as it leads to more logical interpretation --
arrests.eigen$vectors <- -arrests.eigen$vectors
arrests.eigen$vectors
# -- Assign row and column names to the dataframe --
row.names(arrests.eigen$vectors) <- c("Murder", "Assault", "UrbanPop", "Rape")
colnames(arrests.eigen$vectors) <- c("PC1", "PC2", "PC3", "PC4")
arrests.eigen$vectors
# -- Selecting the Number of Principal Components --
# -- Calculate the Proportion of Variance Explained (PVE) --
arrests.eigen$values
PVE <- arrests.eigen$values / sum(arrests.eigen$values)
round(PVE, 2)
# PVE (aka scree) plot
PVEplot <- qplot(c(1:4), PVE) +
geom_line() +
xlab("Principal Component") +
ylab("PVE") +
ggtitle("Scree Plot") +
ylim(0, 1)
# Cumulative PVE plot
cumPVE <- qplot(c(1:4), cumsum(PVE)) +
geom_line() +
xlab("Principal Component") +
ylab(NULL) +
ggtitle("Cumulative Scree Plot") +
ylim(0,1)
grid.arrange(PVEplot, cumPVE, ncol = 2)
# -- Based on the PVE, select only the first two PCs --
phi <- arrests.eigen$vectors[,1:2]
phi
#Step 04 - Transfer datapoints to the new dimensional space
# -- Calculate Principal Components scores --
PC1 <- as.matrix(scaled_df) %*% phi[,1]       # %*% in R does matrix multiplication
PC2 <- as.matrix(scaled_df) %*% phi[,2]
# -- Create a dataframe with Principal Components scores --
PC <- data.frame(State = row.names(USArrests), PC1, PC2)
head(PC)
# -- Plot Principal Components for each State --
ggplot(PC, aes(PC1, PC2)) +
modelr::geom_ref_line(h = 0) +
modelr::geom_ref_line(v = 0) +
geom_text(aes(label = State), size = 3) +
xlab("First Principal Component") +
ylab("Second Principal Component") +
ggtitle("First Two Principal Components of USArrests Data")
library(datasets)
library(datasets)
library(datasets)
k=2
kmeans_vehicle <- kmeans(pre_processed, centers = k, nstart = 10)
vehicleDF <- read.csv("vehicles.csv", row.names = 1)[1:18]
# Load the data
data <- read.csv("F:\Official\w1871471\2 nd year\ML\CW\uow_consumption.csv",skipNul = TRUE)
/
# Load the data
data <- read.csv("F:/Official/w1871471/2 nd year/ML/CW/uow_consumption.csv",skipNul = TRUE)
# Load the data
data <- read.csv("F:/Official/w1871471/2 nd year/ML/CW/uow_consumption.csv",skipNul = TRUE)
data <-data.frame(data)
data <- data[c("date", "X20.00")]
colnames(data)
# Extract the relevant columns (load and hour)
load <- data[, "X20.00"]
hour <- data[, "date"]
# Set the number of time-delayed values to include in the input vector
n <- 4
# Create a matrix of time-delayed load values
input <- matrix(0, nrow = length(load) - n - 7, ncol = n)
for (i in 1:n) {
input[, i] <- load[(n+8-i):(length(load)-i)]
print(input[,i])
}
input
# Create a matrix of time-delayed load values
input <- matrix(0, nrow = length(load) - n - 7, ncol = n)
for (i in 1:n) {
input[, i] <- load[(n+8-i):(length(load)-i)]
print(input)
}
data <- read.csv("F:/Official/w1871471/2 nd year/ML/CW/uow_consumption.csv",skipNul = TRUE)
data <-data.frame(data)
colnames(data)
# load installed package libraries
library(dplyr)
library(readxl)
library(neuralnet)
library(Matrix)
# import and reading the excel file
uow_data <- read_excel("F:/Official/w1871471/2 nd year/ML/CW/Project/NN/uow_consumption.xlsx")
#Getting the data
data_series_20th <- data.frame(uow_data)[,4]
data_series_19th <- data.frame(uow_data)[,3]
data_series_18th <- data.frame(uow_data)[,2]
#Creating I/O matrix with time delayed variables
hour_20 <- bind_cols(G_previous7 = lag(data_series_20th, 7),
G_previous4 = lag(data_series_20th, 4),
G_previous3 = lag(data_series_20th, 3),
G_previous2 = lag(data_series_20th, 2),
G_previous1 = lag(data_series_20th, 1),
G_out = data_series_20th)
hour_19 <- bind_cols(G_previous7 = lag(data_series_19th, 7),
G_previous4 = lag(data_series_19th, 4),
G_previous3 = lag(data_series_19th, 3),
G_previous2 = lag(data_series_19th, 2),
G_previous1 = lag(data_series_19th, 1),
G_out = data_series_20th)
hour_18 <- bind_cols(G_previous7 = lag(data_series_18th, 7),
G_previous4 = lag(data_series_18th, 4),
G_previous3 = lag(data_series_18th, 3),
G_previous2 = lag(data_series_18th, 2),
G_previous1 = lag(data_series_18th, 1),
G_out = data_series_18th)
#Using of complete.cases function to remove those rows with NA
hour_20 <- hour_20[complete.cases(hour_20),]
hour_19 <- hour_19[complete.cases(hour_19),]
hour_18 <- hour_18[complete.cases(hour_18),]
# checking time-delayed I/O configuration from these rows
head (hour_20)
head (hour_19)
head (hour_18)
# function for normalizing the dataset
normalize <- function(x) {
(x - min(x)) / (max(x) - min(x))
}
uow_norm_20 <- as.data.frame(lapply(hour_20, normalize))
uow_norm_19 <- as.data.frame(lapply(hour_19, normalize))
uow_norm_18 <- as.data.frame(lapply(hour_18, normalize))
#Declaring the input and output matrices
io_matrix_3_20th <- uow_norm_20[,1:6]
io_matrix_2_20th <- uow_norm_20[,2:6]
io_matrix_1_20th <- uow_norm_20[,3:6]
io_matrix_3_19th <- uow_norm_19[,1:6]
io_matrix_2_19th <- uow_norm_19[,2:6]
io_matrix_1_19th <- uow_norm_19[,3:6]
io_matrix_3_18th <- uow_norm_18[,1:6]
io_matrix_2_18th <- uow_norm_18[,2:6]
io_matrix_1_18th <- uow_norm_18[,3:6]
#-------------------------------------------------------------------------------------------------------------------
#Training Data set
io_matrix_3_20th_train <- io_matrix_3_20th[1:380, ]
io_matrix_2_20th_train <- io_matrix_2_20th[1:380, ]
io_matrix_1_20th_train <- io_matrix_1_20th[1:380, ]
io_matrix_3_19th_train <- io_matrix_3_19th[1:380, ]
io_matrix_2_19th_train <- io_matrix_2_19th[1:380, ]
io_matrix_1_19th_train <- io_matrix_1_19th[1:380, ]
io_matrix_3_18th_train <- io_matrix_3_18th[1:380, ]
io_matrix_2_18th_train <- io_matrix_2_18th[1:380, ]
io_matrix_1_18th_train <- io_matrix_1_18th[1:380, ]
#Testing Data set
io_matrix_3_20th_test <- io_matrix_3_20th[381:nrow(io_matrix_3_20th), ]
io_matrix_2_20th_test <- io_matrix_2_20th[381:nrow(io_matrix_2_20th), ]
io_matrix_1_20th_test <- io_matrix_1_20th[381:nrow(io_matrix_1_20th), ]
io_matrix_3_19th_test <- io_matrix_3_19th[381:nrow(io_matrix_3_19th), ]
io_matrix_2_19th_test <- io_matrix_2_19th[381:nrow(io_matrix_2_19th), ]
io_matrix_1_19th_test <- io_matrix_1_19th[381:nrow(io_matrix_1_19th), ]
io_matrix_3_18th_test <- io_matrix_3_18th[381:nrow(io_matrix_3_18th), ]
io_matrix_2_18th_test <- io_matrix_2_18th[381:nrow(io_matrix_2_18th), ]
io_matrix_1_18th_test <- io_matrix_1_18th[381:nrow(io_matrix_1_18th), ]
#-------------------------------------------------------------------------------------------------------
# creating models of 18th hour
nn_model_18th_3_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_3_18th_train, linear.output=FALSE)
nn_model_18th_3_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_3_18th_train ,act.fct = "logistic", linear.output=FALSE)
nn_model_18th_2_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_2_18th_train, linear.output=FALSE)
nn_model_18th_2_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_2_18th_train ,act.fct = "logistic", linear.output=FALSE)
nn_model_18th_1_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_1_18th_train, linear.output=FALSE)
nn_model_18th_1_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_1_18th_train ,act.fct = "logistic", linear.output=FALSE)
#------------------------------------------------------------------------------------------------------
# predicting the output of 18th hour, base on given training data
pred_rslt_18th_3_m1 <- compute(nn_model_18th_3_m1, io_matrix_3_18th_train[,1:5])
pred_rslt_18th_3_m2 <- compute(nn_model_18th_3_m2, io_matrix_3_18th_train[,1:5])
pred_rslt_18th_2_m1 <- compute(nn_model_18th_2_m1, io_matrix_2_18th_train[,1:4])
pred_rslt_18th_2_m2 <- compute(nn_model_18th_2_m2, io_matrix_2_18th_train[,1:4])
pred_rslt_18th_1_m1 <- compute(nn_model_18th_1_m1, io_matrix_1_18th_train[,1:3])
pred_rslt_18th_1_m2 <- compute(nn_model_18th_1_m2, io_matrix_1_18th_train[,1:3])
#-------------------------------------------------------------------------------------------------------
# creating again 19th hour train matrix
io_matrix_3_19th_m1_train <- cbind(io_matrix_3_19th_train[,1:5], pred_rslt_18th_3_m1$net.result, io_matrix_3_19th_train["G_out"])
io_matrix_3_19th_m2_train <- cbind(io_matrix_3_19th_train[,1:5], pred_rslt_18th_3_m2$net.result, io_matrix_3_19th_train["G_out"])
io_matrix_2_19th_m1_train <- cbind(io_matrix_2_19th_train[,1:4], pred_rslt_18th_2_m1$net.result, io_matrix_2_19th_train["G_out"])
io_matrix_2_19th_m2_train <- cbind(io_matrix_2_19th_train[,1:4], pred_rslt_18th_2_m2$net.result, io_matrix_2_19th_train["G_out"])
io_matrix_1_19th_m1_train <- cbind(io_matrix_1_19th_train[,1:3], pred_rslt_18th_1_m1$net.result, io_matrix_1_19th_train["G_out"])
io_matrix_1_19th_m2_train <- cbind(io_matrix_1_19th_train[,1:3], pred_rslt_18th_1_m2$net.result, io_matrix_1_19th_train["G_out"])
colnames(io_matrix_3_19th_m1_train)[6] <- "G_previous8"
colnames(io_matrix_3_19th_m2_train)[6] <- "G_previous8"
colnames(io_matrix_2_19th_m1_train)[5] <- "G_previous8"
colnames(io_matrix_2_19th_m2_train)[5] <- "G_previous8"
colnames(io_matrix_1_19th_m1_train)[4] <- "G_previous8"
colnames(io_matrix_1_19th_m2_train)[4] <- "G_previous8"
# creating models of 19th hour
nn_model_19th_3_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_3_19th_m1_train, linear.output=FALSE)
nn_model_19th_3_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_3_19th_m2_train ,act.fct = "logistic", linear.output=FALSE)
nn_model_19th_2_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_2_19th_m1_train, linear.output=FALSE)
nn_model_19th_2_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_2_19th_m2_train ,act.fct = "logistic", linear.output=FALSE)
nn_model_19th_1_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_1_19th_m1_train, linear.output=FALSE)
nn_model_19th_1_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_1_19th_m2_train ,act.fct = "logistic", linear.output=FALSE)
# predicting the output of 19th hour, base on given training data
pred_rslt_19th_3_m1 <- compute(nn_model_19th_3_m1, io_matrix_3_19th_m1_train[,1:6])
pred_rslt_19th_3_m2 <- compute(nn_model_19th_3_m2, io_matrix_3_19th_m2_train[,1:6])
pred_rslt_19th_2_m1 <- compute(nn_model_19th_2_m1, io_matrix_2_19th_m1_train[,1:5])
pred_rslt_19th_2_m2 <- compute(nn_model_19th_2_m2, io_matrix_2_19th_m2_train[,1:5])
pred_rslt_19th_1_m1 <- compute(nn_model_19th_1_m1, io_matrix_1_19th_m1_train[,1:4])
pred_rslt_19th_1_m2 <- compute(nn_model_19th_1_m2, io_matrix_1_19th_m2_train[,1:4])
#-------------------------------------------------------------------------------------------------------
# creating again 20th hour train matrix
io_matrix_3_20th_m1_train <- cbind(io_matrix_3_20th_train[,1:5], pred_rslt_19th_3_m1$net.result, io_matrix_3_20th_train["G_out"])
io_matrix_3_20th_m2_train <- cbind(io_matrix_3_20th_train[,1:5], pred_rslt_19th_3_m2$net.result, io_matrix_3_20th_train["G_out"])
io_matrix_2_20th_m1_train <- cbind(io_matrix_2_20th_train[,1:4], pred_rslt_19th_2_m1$net.result, io_matrix_2_20th_train["G_out"])
io_matrix_2_20th_m2_train <- cbind(io_matrix_2_20th_train[,1:4], pred_rslt_19th_2_m2$net.result, io_matrix_2_20th_train["G_out"])
io_matrix_1_20th_m1_train <- cbind(io_matrix_1_20th_train[,1:3], pred_rslt_19th_1_m1$net.result, io_matrix_1_20th_train["G_out"])
io_matrix_1_20th_m2_train <- cbind(io_matrix_1_20th_train[,1:3], pred_rslt_19th_1_m2$net.result, io_matrix_1_20th_train["G_out"])
colnames(io_matrix_3_20th_m1_train)[6] <- "G_previous9"
colnames(io_matrix_3_20th_m2_train)[6] <- "G_previous9"
colnames(io_matrix_2_20th_m1_train)[5] <- "G_previous9"
colnames(io_matrix_2_20th_m2_train)[5] <- "G_previous9"
colnames(io_matrix_1_20th_m1_train)[4] <- "G_previous9"
colnames(io_matrix_1_20th_m2_train)[4] <- "G_previous9"
# creating models of 20th hour
nn_model_20th_3_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_3_20th_m1_train, linear.output=FALSE)
nn_model_20th_3_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_3_20th_m2_train ,act.fct = "logistic", linear.output=FALSE)
nn_model_20th_2_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_2_20th_m1_train, linear.output=FALSE)
nn_model_20th_2_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_2_20th_m2_train ,act.fct = "logistic", linear.output=FALSE)
nn_model_20th_1_m1 <- neuralnet(G_out ~.
, hidden = 4, data = io_matrix_1_20th_m1_train, linear.output=FALSE)
nn_model_20th_1_m2 <- neuralnet(G_out ~.
, hidden = c(4,8), data = io_matrix_1_20th_m2_train ,act.fct = "logistic", linear.output=FALSE)
#TESTING-----------------------------------------------------------------------------------------------
#------------------------------------------------------------------------------------------------------
# predicting the output of 18th hour, base on given testing data
pred_rslt_18th_3_m1_test <- compute(nn_model_18th_3_m1, io_matrix_3_18th_test[,1:5])
pred_rslt_18th_3_m2_test <- compute(nn_model_18th_3_m2, io_matrix_3_18th_test[,1:5])
pred_rslt_18th_2_m1_test <- compute(nn_model_18th_2_m1, io_matrix_2_18th_test[,1:4])
pred_rslt_18th_2_m2_test <- compute(nn_model_18th_2_m2, io_matrix_2_18th_test[,1:4])
pred_rslt_18th_1_m1_test <- compute(nn_model_18th_1_m1, io_matrix_1_18th_test[,1:3])
pred_rslt_18th_1_m2_test <- compute(nn_model_18th_1_m2, io_matrix_1_18th_test[,1:3])
#-------------------------------------------------------------------------------------------------------
# creating again 19th hour test matrix
io_matrix_3_19th_m1_test <- cbind(io_matrix_3_19th_test[,1:5], pred_rslt_18th_3_m1_test$net.result, io_matrix_3_19th_test["G_out"])
io_matrix_3_19th_m2_test <- cbind(io_matrix_3_19th_test[,1:5], pred_rslt_18th_3_m2_test$net.result, io_matrix_3_19th_test["G_out"])
io_matrix_2_19th_m1_test <- cbind(io_matrix_2_19th_test[,1:4], pred_rslt_18th_2_m1_test$net.result, io_matrix_2_19th_test["G_out"])
io_matrix_2_19th_m2_test <- cbind(io_matrix_2_19th_test[,1:4], pred_rslt_18th_2_m2_test$net.result, io_matrix_2_19th_test["G_out"])
io_matrix_1_19th_m1_test <- cbind(io_matrix_1_19th_test[,1:3], pred_rslt_18th_1_m1_test$net.result, io_matrix_1_19th_test["G_out"])
io_matrix_1_19th_m2_test <- cbind(io_matrix_1_19th_test[,1:3], pred_rslt_18th_1_m2_test$net.result, io_matrix_1_19th_test["G_out"])
colnames(io_matrix_3_19th_m1_test)[6] <- "G_previous8"
colnames(io_matrix_3_19th_m2_test)[6] <- "G_previous8"
colnames(io_matrix_2_19th_m1_test)[5] <- "G_previous8"
colnames(io_matrix_2_19th_m2_test)[5] <- "G_previous8"
colnames(io_matrix_1_19th_m1_test)[4] <- "G_previous8"
colnames(io_matrix_1_19th_m2_test)[4] <- "G_previous8"
# predicting the output of 19th hour, base on given training data
pred_rslt_19th_3_m1 <- compute(nn_model_19th_3_m1, io_matrix_3_19th_m1_test[,1:6])
pred_rslt_19th_3_m2 <- compute(nn_model_19th_3_m2, io_matrix_3_19th_m2_test[,1:6])
pred_rslt_19th_2_m1 <- compute(nn_model_19th_2_m1, io_matrix_2_19th_m1_test[,1:5])
pred_rslt_19th_2_m2 <- compute(nn_model_19th_2_m2, io_matrix_2_19th_m2_test[,1:5])
pred_rslt_19th_1_m1 <- compute(nn_model_19th_1_m1, io_matrix_1_19th_m1_test[,1:4])
pred_rslt_19th_1_m2 <- compute(nn_model_19th_1_m2, io_matrix_1_19th_m2_test[,1:4])
#-------------------------------------------------------------------------------------------------------
# creating again 20th hour test matrix
io_matrix_3_20th_m1_test <- cbind(io_matrix_3_20th_test[,1:5], pred_rslt_19th_3_m1$net.result, io_matrix_3_20th_test["G_out"])
io_matrix_3_20th_m2_test <- cbind(io_matrix_3_20th_test[,1:5], pred_rslt_19th_3_m2$net.result, io_matrix_3_20th_test["G_out"])
io_matrix_2_20th_m1_test <- cbind(io_matrix_2_20th_test[,1:4], pred_rslt_19th_2_m1$net.result, io_matrix_2_20th_test["G_out"])
io_matrix_2_20th_m2_test <- cbind(io_matrix_2_20th_test[,1:4], pred_rslt_19th_2_m2$net.result, io_matrix_2_20th_test["G_out"])
io_matrix_1_20th_m1_test <- cbind(io_matrix_1_20th_test[,1:3], pred_rslt_19th_1_m1$net.result, io_matrix_1_20th_test["G_out"])
io_matrix_1_20th_m2_test <- cbind(io_matrix_1_20th_test[,1:3], pred_rslt_19th_1_m2$net.result, io_matrix_1_20th_test["G_out"])
colnames(io_matrix_3_20th_m1_test)[6] <- "G_previous9"
colnames(io_matrix_3_20th_m2_test)[6] <- "G_previous9"
colnames(io_matrix_2_20th_m1_test)[5] <- "G_previous9"
colnames(io_matrix_2_20th_m2_test)[5] <- "G_previous9"
colnames(io_matrix_1_20th_m1_test)[4] <- "G_previous9"
colnames(io_matrix_1_20th_m2_test)[4] <- "G_previous9"
# predicting the output of 20th hour, base on given training data
pred_rslt_20th_3_m1_test <- compute(nn_model_20th_3_m1, io_matrix_3_20th_test[,1:6])
pred_rslt_20th_3_m2_test <- compute(nn_model_20th_3_m2, io_matrix_3_20th_test[,1:6])
pred_rslt_20th_2_m1_test <- compute(nn_model_20th_2_m1, io_matrix_2_20th_test[,1:5])
pred_rslt_20th_2_m2_test <- compute(nn_model_20th_2_m2, io_matrix_2_20th_test[,1:5])
pred_rslt_20th_1_m1_test <- compute(nn_model_20th_1_m1, io_matrix_1_20th_test[,1:4])
pred_rslt_20th_1_m2_test <- compute(nn_model_20th_1_m2, io_matrix_1_20th_test[,1:4])
# and find its maximum & minimum value
min_val_20th <- min(data_series_20th)
max_val_20th <- max(data_series_20th)
min_val_19th <- min(data_series_19th)
max_val_19th <- max(data_series_19th)
min_val_18th <- min(data_series_18th)
max_val_18th <- max(data_series_18th)
#Create the reverse of normalised function – de-normalized
unnormalize <- function(x, min, max) {
return( (max - min)*x + min )
}
predicted_3_m1 <- unnormalize(pred_rslt_20th_3_m1_test$net.result, min_val_20th, max_val_20th)
predicted_3_m2 <- unnormalize(pred_rslt_20th_3_m2_test$net.result, min_val_20th, max_val_20th)
predicted_2_m1 <- unnormalize(pred_rslt_20th_2_m1_test$net.result, min_val_20th, max_val_20th)
predicted_2_m2 <- unnormalize(pred_rslt_20th_2_m2_test$net.result, min_val_20th, max_val_20th)
predicted_1_m1 <- unnormalize(pred_rslt_20th_1_m1_test$net.result, min_val_20th, max_val_20th)
predicted_1_m2 <- unnormalize(pred_rslt_20th_1_m2_test$net.result, min_val_20th, max_val_20th)
actual20th_3 <- unnormalize(io_matrix_3_20th_test["G_out"], min_val_20th, max_val_20th)
actual20th_2 <- unnormalize(io_matrix_2_20th_test["G_out"], min_val_20th, max_val_20th)
actual20th_1 <- unnormalize(io_matrix_1_20th_test["G_out"], min_val_20th, max_val_20th)
library(Metrics)
evaluation <- function(actual, predicted){
actual <- unlist(actual)
predicted <- unlist(predicted)
rmse <- rmse(actual, predicted)
message("rmse :",rmse)
mae  <- mae(actual, predicted)
message("mae :",mae)
mape <- mape(actual, predicted)
message("mape :",mape)
smape<- smape(actual, predicted)
message("smape :",smape)
}
table_3_m1 <- cbind(predicted_3_m1, actual20th_3)
table_3_m2 <- cbind(predicted_3_m2, actual20th_3)
table_2_m1 <- cbind(predicted_2_m2, actual20th_2)
table_2_m2 <- cbind(predicted_2_m2, actual20th_2)
table_1_m1 <- cbind(predicted_1_m2, actual20th_1)
table_1_m2 <- cbind(predicted_1_m2, actual20th_1)
colnames(table_3_m1)[1] <- "Predicted"
colnames(table_3_m2)[1] <- "Predicted"
colnames(table_2_m1)[1] <- "Predicted"
colnames(table_2_m2)[1] <- "Predicted"
colnames(table_1_m1)[1] <- "Predicted"
colnames(table_1_m2)[1] <- "Predicted"
rownames(table_3_m1) <- 1:nrow(table_3_m1)
rownames(table_3_m2) <- 1:nrow(table_3_m2)
rownames(table_2_m1) <- 1:nrow(table_2_m1)
rownames(table_2_m2) <- 1:nrow(table_2_m2)
rownames(table_1_m1) <- 1:nrow(table_1_m1)
rownames(table_1_m2) <- 1:nrow(table_1_m2)
print(head(table_3_m1))
evaluation(predicted_3_m1, actual20th_3)
print(head(table_3_m2))
evaluation(predicted_3_m2, actual20th_3)
print(head(table_2_m1))
evaluation(predicted_2_m1, actual20th_2)
print(head(table_2_m2))
evaluation(predicted_2_m2, actual20th_2)
print(head(table_1_m1))
evaluation(predicted_1_m1, actual20th_1)
print(head(table_1_m2))
evaluation(predicted_1_m2, actual20th_1)
library(NbClust)
library(dplyr)
library(tidyr)
library(ggplot2)
library(factoextra)
library(cluster)
script_path <- dirname(rstudioapi::getSourceEditorContext()$path);
setwd(script_path)
################################ Substask 2 ####################################
############################### PART E. PCA ####################################
vehicleDF <- read.csv("vehicles.csv", row.names = 1)[1:18]
# Create empty list to store outlier row indices
outlier_rows <- list()
# Loop through columns and extract outlier row indices
boxplot(vehicleDF, plot = TRUE)
for (col in names(vehicleDF)) {
outliers <- boxplot(vehicleDF[[col]], plot = FALSE)$out
row_indices <- which(vehicleDF[[col]] %in% outliers)
if (length(row_indices) > 0) {
outlier_rows[[col]] <- as.integer(row_indices)
}
}
outlier_rows <- unique(unlist(outlier_rows))
vehicleDF <- vehicleDF[-outlier_rows, ]
boxplot(vehicleDF, plot = TRUE)
# compute variance of each variable
apply(vehicleDF, 2, var)
# Apply PCA using prcomp()
pca <- prcomp(vehicleDF, center = TRUE, scale = TRUE)
pca_summary <- summary(pca, loadings=TRUE)
print(summary(pca))
# Extract the eigenvalues and eigenvector
(eigenvalues <- pca$sdev^2)
(eigenvectors <- data.frame(pca$rotation))
# Extract the cumulative proportion values
proportion_of_Variance <- data.frame(pca_summary$importance)[2,]
plot(1:length(proportion_of_Variance), proportion_of_Variance, type = "b",
xlab = "Principal Component", ylab = "Proportion of Variance Explained",
main = "Scree Plot")
abline(h = 0.92, lty = 2)
# if we used the first 6 components we would be able to account for >92% of total variance in the data.
# we are avoiding including too many components, which could result in overfitting or loss of interpretability.
# find the index of PC's that crosses the 0.92(92%) threshold
pc_indexes = list()
sorted_values <- sort(pca_summary$importance[2,], decreasing = TRUE)
sum <- 0
row <- proportion_of_Variance[1,]
index_of_list <- 1
for (i in sorted_values) {
sum <- sum + i
index <- which(row == i)
pc_indexes[[index_of_list]] <- index
index_of_list <- index_of_list +1
if(sum>=0.92){
break
}
}
pc_indexes <- as.integer(unlist(pc_indexes))
transformed <- data.frame(-pca$x)[pc_indexes]
########################### PART F. cchecking best K value #####################
### NbClust Method
set.seed(30)
#clusterNoE = NbClust(transformed, distance = "euclidean", min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
#clusterNoM = NbClust(transformed, distance = "manhattan", min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
clusterNoMx = NbClust(transformed, distance = "maximum", min.nc = 2, max.nc = 10, method = "kmeans", index = "all")
